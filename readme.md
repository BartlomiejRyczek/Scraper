# Opis
Projekt sÅ‚uÅ¼y do pobierania ogÅ‚oszeÅ„ o pracÄ™ z portali Just Join IT oraz Pracuj.pl, zapisywania ich do plikÃ³w CSV, a nastÄ™pnie przetwarzania (m.in. czyszczenie danych, analiza statystyczna) w jÄ™zyku Python.

# Struktura
* scraper_just_join.py â€“ Skrypt Selenium + BeautifulSoup do pobierania i zapisu ofert z justjoin.it
* scraper_pracuj_pl.py â€“ Skrypt Selenium + BeautifulSoup do pobierania i zapisu ofert z it.pracuj.pl
* analizaDanych.ipynb â€“ Notebook z przykÅ‚adami obrÃ³bki, wizualizacji i analizy danych (np. korelacje, statystyki opisowe)


# Kroki dziaÅ‚ania

1. Uruchom skrypty scraperÃ³w (scraper_just_join.py oraz scraper_pracuj_pl.py) â€“ dane trafiÄ… do plikÃ³w CSV (np. job_offers_just_join.csv, job_offers.csv).
2. OtwÃ³rz plik analizaDanych.ipynb w Å›rodowisku Jupyter/VS Code, by przeprowadziÄ‡ dalszÄ… analizÄ™ (np. wykresy, korelacje, testy).


# Wymagania
Python 3

Biblioteki: requests, BeautifulSoup, pandas, NumPy, Selenium, Matplotlib, re (oraz inne uÅ¼yte w notebooku)
Zainstalowany web driver do Selenium (np. ChromeDriver)

# Uruchamianie 
1. Zainstaluj wymagane pakiety: 
``` bash
pip install -r requirements.txt
```

2. Upewnij siÄ™, Å¼e masz zainstalowany i skonfigurowany ChromeDriver.
3. Uruchom terminal i wykonaj:

``` bash
python scraper_just_join.py
python scraper_pracuj_pl.py
```

4. Zaimportuj lub otwÃ³rz plik analizaDanych.ipynb w swoimi ulubionym edytorze (np. VS Code) i uruchom kolejne komÃ³rki.

# Uwagi
* W skryptach uÅ¼ywany jest mechanizm porÃ³wnywania rekordÃ³w, by unikaÄ‡ duplikatÃ³w â€“ sprawdzaj klucze unikalnoÅ›ci (np. (TytuÅ‚, Firma, Link)).
* W plikach CSV przechowywane sÄ… m.in. tytuÅ‚ oferty, firma, data, wynagrodzenie, technologie oraz link.
* Plik convert_to_tidy_data.py nie jest potrzebny do prawidÅ‚owego dziaÅ‚ania programu (Jest w trakcie rozwoju)



# ğŸ“Œ Automatyczny Web Scraper Ofert Pracy IT

## ğŸ“Œ Opis projektu
Ten projekt zawiera **automatyczny scraper** do pobierania ofert pracy IT z dwÃ³ch popularnych portali:
- **Pracuj.pl**
- **JustJoin.it**

Dane sÄ… przetwarzane, filtrowane i zapisywane do plikÃ³w CSV, aby umoÅ¼liwiÄ‡ pÃ³ÅºniejszÄ… analizÄ™.

---

## ğŸš€ Funkcje projektu
âœ… **ObsÅ‚uga dynamicznych stron** - Selenium w tle przewija stronÄ™, aby pobraÄ‡ wszystkie dostÄ™pne oferty. 
âœ… **Automatyczne pobieranie ofert pracy** - skrypty mogÄ… automatycznie uruchamiaÄ‡ siÄ™ przy kaÅ¼dym starcie komputera, gdy plik BAT zostanie umieszczony w folderze autostartu.
âœ… **Zapis do CSV** - pobrane dane sÄ… przechowywane w pliku `job_offerts.csv` oraz `job_offerts_just_join.csv`.   
âœ… **Unikanie duplikatÃ³w** - w notebooku eliminuje siÄ™ duplikaty oraz scala dane
âœ… **Zapis do CSV** - ujednolicone dane sÄ… przechowywane w pliku `oferty_pracy_bez_duplikatow.csv`.  
âœ… **ObsÅ‚uga wynagrodzeÅ„ i technologii** - ekstrakcja technologii i wideÅ‚ek pÅ‚acowych.  
âœ… **Analiza danych** - statystyki, wykresy i analiza trendÃ³w.


---

## ğŸ›  Wymagania
ğŸ”¹ **Python 3.x**  
ğŸ”¹ **Selenium**  
ğŸ”¹ **BeautifulSoup**  
ğŸ”¹ **Pandas**  
ğŸ”¹ **Matplotlib** 

Aby zainstalowaÄ‡ wymagane biblioteki, uruchom:
```bash
pip install -r requirements.txt
```

---

## âš™ Instalacja i uruchomienie
### **1ï¸âƒ£ Uruchamianie rÄ™czne skryptÃ³w**
Uruchom scraper dla Pracuj.pl:
```bash
python scraper_pracuj_pl.py
```
Uruchom scraper dla JustJoin.it:
```bash
python scraper_just_join.py
```

### **3ï¸âƒ£ Automatyczne uruchamianie przy starcie systemu**
Uruchom plik `runScrapers.bat`, ktÃ³ry automatycznie uruchomi oba skrypty (pamiÄ™taj o zmienieniu Å›cieÅ¼ki, by byÅ‚a poprawna)
Polecam dodaÄ‡ rÃ³wnieÅ¼ ten plik do autostarty systemu, w celu automatycznego scrapowania danych przy kazdym starcie komputera. 

---

## ğŸ“Š Analiza danych
Dane sÄ… analizowane w pliku **analizaDanych.ipynb**, gdzie moÅ¼na:
- ğŸ† **ZbadaÄ‡ Å›rednie wynagrodzenie w rÃ³Å¼nych technologiach**
- ğŸ“ˆ **WykonaÄ‡ analizÄ™ trendÃ³w iloÅ›ci ofert w czasie**
- ğŸ“Š **ZwizualizowaÄ‡ rozkÅ‚ad wynagrodzeÅ„**
- ğŸ” **SprawdziÄ‡, ktÃ³re technologie sÄ… najczÄ™Å›ciej wymagane**

---

## ğŸš€ Potencjalne ulepszenia projektu
ğŸ”¹ **UÅ¼ycie bazy danych (SQLite/PostgreSQL) zamiast CSV**  
ğŸ”¹ **Lepsza obsÅ‚uga bÅ‚Ä™dÃ³w i logowanie (`logging`)**  
ğŸ”¹ **Scrapowanie asynchroniczne dla lepszej wydajnoÅ›ci (`Asyncio`, `Scrapy`)**  
ğŸ”¹ **Dodanie analizy lokalizacji ofert i generowanie mapy (Folium)**  
ğŸ”¹ **Wizualizacja trendÃ³w w czasie (np. popularnoÅ›Ä‡ jÄ™zykÃ³w programowania)**  
ğŸ”¹ **Integracja z AI (np. analiza treÅ›ci ofert przez GPT)**


# âš ï¸ Uwagi
* W skryptach uÅ¼ywany jest mechanizm porÃ³wnywania rekordÃ³w, by unikaÄ‡ duplikatÃ³w â€“ sprawdza klucze unikalnoÅ›ci (TytuÅ‚, Firma, Link).
* W plikach CSV przechowywane sÄ… m.in. tytuÅ‚ oferty, firma, data, wynagrodzenie, technologie oraz link.
* Plik convert_to_tidy_data.py nie jest potrzebny do prawidÅ‚owego dziaÅ‚ania programu (Jest w trakcie rozwoju)